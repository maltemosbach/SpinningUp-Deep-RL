# Vanilla Policy Gradient (VPG)
## Overview
In vanilla policy gradient (VPG) we train a stochastic policy <img class="math" src="../../docs/images/VPG/pi_theta.svg" alt="\pi_{\theta}"/> or <img class="math" src="../../docs/images/VPG/pi_theta_125.svg" alt="\pi_{\theta}"/> or <img class="math" src="../../docs/images/VPG/pi_theta_150.svg" alt="\pi_{\theta}"/> in an on policy way to maximize the performance J(pi_theta).


<img align="left" width="100" height="100" src="../../docs/images/VPG/pi_theta.svg">

<img class="math" src="../../docs/images/VPG/pi_theta.svg" alt="\pi_{\theta}"/>





<img align="left" width="100" height="100" src="http://1.bp.blogspot.com/_LgF7ePXTRlA/TTXdqhZLHcI/AAAAAAAAAEg/M2ya6KBz61E/s1600/apple_logo_rainbow.gif">

<img class="math" src="http://1.bp.blogspot.com/_LgF7ePXTRlA/TTXdqhZLHcI/AAAAAAAAAEg/M2ya6KBz61E/s1600/apple_logo_rainbow.gif" alt="\pi_{\theta}"/>

## Pseudocode
![VPG pseudocode algorithm](../../docs/images/VPG/VPG_Pseudocode.svg)
