# Vanilla Policy Gradient (VPG)

## Overview
In vanilla policy gradient (VPG) we train a stochastic policy <img align="center" src="../../docs/images/VPG/pi_theta.svg" alt="\pi_{\theta}"/> or <img align="center" src="../../docs/images/VPG/pi_theta_125.svg" alt="\pi_{\theta}"/> or <img align="center" src="../../docs/images/VPG/pi_theta_150.svg" alt="\pi_{\theta}"/> or <img class="math" src="../../docs/images/VPG/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg" alt="\pi_{\theta}" vertical-align="middle"/> in an on policy way to maximize the performance J(pi_theta).






## Pseudocode
![VPG pseudocode algorithm](../../docs/images/VPG/VPG_Pseudocode.svg)
